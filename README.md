
## PCA-Guided Compression for Neural Models

The core principle is simple: compress the input data as much as possible so that the resulting model can be as small as possible. To preserve or even improve inference performance, the preprocessing step must be handled efficiently â€” ideally on GPU.

In the [associated arXiv paper](https://arxiv.org/abs/2508.04307), we present three models where this approach is applied. This repository contains the full results from all three case studies, including executed notebooks with output.

The notebook is clean and executable, and shows the latest run to ensure reproducibility and transparency.

